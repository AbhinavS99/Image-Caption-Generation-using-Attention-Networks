{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vocabulary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSF7ytP3T0R6",
        "outputId": "716a2a86-0482-48f1-9f23-f9e5ca60b833"
      },
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import pickle as pkl\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "train_data_captions = pkl.load(open(\"/content/drive/MyDrive/Data/Train/train_captions.pkl\",\"rb\"))\n",
        "val_data_captions = pkl.load(open(\"/content/drive/MyDrive/Data/Val/val_captions.pkl\",\"rb\"))\n",
        "test_data_captions = pkl.load(open(\"/content/drive/MyDrive/Data/Test/test_captions.pkl\",\"rb\"))\n",
        "\n",
        "sentence_list = []\n",
        "\n",
        "for idx in train_data_captions:\n",
        "    for caption in train_data_captions[idx]:\n",
        "        sentence_list.append(caption)\n",
        "\n",
        "for idx in val_data_captions:\n",
        "    for caption in val_data_captions[idx]:\n",
        "        sentence_list.append(caption)\n",
        "\n",
        "for idx in test_data_captions:\n",
        "    for caption in test_data_captions[idx]:\n",
        "        sentence_list.append(caption)\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for sentence in sentence_list:\n",
        "\tcounter.update((re.sub(r'[^\\w\\s]', '', sentence).lower()).split())\n",
        "\n",
        "vocab = Vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "\n",
        "class TextNumericalizer():\n",
        "\tdef __init__(self, vocab, tokenizer):\n",
        "\t\tself.vocab = vocab\n",
        "\n",
        "\tdef tokenize(self, sentence):\n",
        "\t\treturn (re.sub(r'[^\\w\\s]', '', sentence).lower()).split(\" \")\n",
        "\n",
        "\tdef SentenceToVector(self, sentence):\n",
        "\t\treturn [self.vocab.stoi[token.lower()] for token in self.tokenize(sentence)]\n",
        "\n",
        "\tdef VectorToSentence(self, vector):\n",
        "\t\treturn [self.vocab.itos[integer] for integer in vector]\n",
        "  \n",
        "\tdef getVocabularyLength(self):\n",
        "\t\treturn len(self.vocab.stoi.keys())\n",
        "\n",
        "TN = TextNumericalizer(vocab, tokenizer)\n",
        "\n",
        "pkl.dump(TN, open('/content/drive/MyDrive/Data/TextNumericalizer.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}