{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl7hLlLANPZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4296f51b-39fa-4913-d6b4-27ddb5b926c6"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.transforms as T\n",
        "from copy import deepcopy\n",
        "import re\n",
        "from google.colab import drive\n",
        "import pickle as pkl\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TextNumericalizer():\n",
        "\tdef __init__(self, vocab, tokenizer):\n",
        "\t\tself.vocab = vocab\n",
        "\n",
        "\tdef tokenize(self, sentence):\n",
        "\t\treturn (re.sub(r'[^\\w\\s]', '', sentence).lower()).split(\" \")\n",
        "\n",
        "\tdef SentenceToVector(self, sentence):\n",
        "\t\treturn [self.vocab.stoi[token.lower()] for token in self.tokenize(sentence)]\n",
        "\n",
        "\tdef VectorToSentence(self, vector):\n",
        "\t\treturn [self.vocab.itos[integer] for integer in vector]\n",
        "  \n",
        "\tdef getVocabularyLength(self):\n",
        "\t\treturn len(self.vocab.stoi.keys())\n",
        "\n",
        "class ImgCapDataset(Dataset):\n",
        "    def __init__(self, X, Y, TN, transform=None):\n",
        "        assert len(X)==len(Y), \"Data should be of the same length! [Error: X(\" + str(len(X)) + \") != Y(\" + str(lenY) + \")]\"\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.TN = TN\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        temp = deepcopy(self.Y[index])\n",
        "        for i in range(len(self.Y[index])):\n",
        "            temp[i] = re.sub(r'[^\\w\\s]', '', temp[i]).lower()\n",
        "\n",
        "        return self.transform(self.X[index]), torch.tensor([self.TN.vocab.stoi[\"<sos>\"]]+self.TN.SentenceToVector(temp[0])+[self.TN.vocab.stoi[\"<eos>\"]], dtype=torch.long), temp\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "class CollateFunction():\n",
        "    def __init__(self, padding):\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return torch.cat([i[0].unsqueeze(0) for i in batch], dim=0), pad_sequence([i[1] for i in batch], batch_first=True, padding_value=self.padding), [i[2] for i in batch]\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader = pkl.load(open(\"/content/drive/MyDrive/Data/Train/train_loader.pkl\",\"rb\"))\n",
        "\n",
        "# Encoder Network\n",
        "class ENet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ENet, self).__init__()\n",
        "        temp = torchvision.models.vgg19(pretrained=True)\n",
        "\n",
        "        # No Fine-Tuning Of The Pre-Trained Model\n",
        "        for p in temp.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.encoder = torch.nn.Sequential(*list(temp.features.children())[:-2])\n",
        "\n",
        "        for c in list(self.encoder.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "        self.feats = torch.nn.AdaptiveAvgPool2d((14,14))\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        enc = self.feats(enc)\n",
        "        enc = enc.permute(0, 2, 3, 1)\n",
        "        enc = enc.view(-1, 196, 512)\n",
        "\n",
        "        return enc\n",
        "\n",
        "# Bahdanau Attention Network\n",
        "class ANet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANet, self).__init__()\n",
        "        self.v = torch.nn.Linear(512, 1)\n",
        "        self.W = torch.nn.Linear(512, 512)\n",
        "        self.U = torch.nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, feats, h_state):\n",
        "        U = self.U(h_state).unsqueeze(dim=1)\n",
        "        W = self.W(feats)\n",
        "        A = torch.nn.functional.tanh(W+U)\n",
        "        e = self.v(A).squeeze(dim=2)\n",
        "        alpha = torch.nn.functional.softmax(e, dim=1)\n",
        "        context_vector = (feats*alpha.unsqueeze(dim=2)).sum(dim=1)\n",
        "\n",
        "        return alpha, context_vector\n",
        "\n",
        "# Decoder Network With Bahdanau Attention Mechanism\n",
        "class DNet(torch.nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(DNet, self).__init__()\n",
        "        self.v_size = vocab_size\n",
        "        self.ANet = ANet()\n",
        "        self.init_h = torch.nn.Linear(512, 512)\n",
        "        self.init_c = torch.nn.Linear(512, 512)\n",
        "        self.f_beta = torch.nn.Linear(512, 512)\n",
        "        self.output_layer = torch.nn.Linear(512, vocab_size)\n",
        "        self.embedding_layer = torch.nn.Embedding(vocab_size, 512)\n",
        "        self.LSTM = torch.nn.LSTMCell(1024, 512)\n",
        "\n",
        "    def forward(self, feats, caps):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        avg = torch.mean(feats, dim=1)\n",
        "        h = torch.nn.functional.tanh(self.init_h(avg))\n",
        "        c = torch.nn.functional.tanh(self.init_c(avg))\n",
        "        T = len(caps[0])-1\n",
        "        embedded_caps = None\n",
        "\n",
        "        if(self.training):\n",
        "            embedded_caps = self.embedding_layer(caps)\n",
        "        else:\n",
        "            embedded_caps = self.embedding_layer(torch.zeros(feats.size(0), 1).long().to(device))\n",
        "\n",
        "        sentences = torch.zeros((feats.size(0), T, self.v_size)).to(device)\n",
        "        weights = torch.zeros((feats.size(0), T, 196)).to(device)\n",
        "\n",
        "        for t in range(T):\n",
        "            alpha, context_vector = self.ANet(feats, h)\n",
        "            gated_context_vector = torch.nn.functional.sigmoid(self.f_beta(h))*context_vector\n",
        "            input = None\n",
        "\n",
        "            if(self.training):\n",
        "                input = torch.cat((embedded_caps[:, t], gated_context_vector), dim=1)\n",
        "            else:\n",
        "                embedded_caps = embedded_caps.squeeze(1) if embedded_caps.dim()==3 else embedded_caps\n",
        "                input = torch.cat((embedded_caps, gated_context_vector), dim=1)\n",
        "\n",
        "            h, c = self.LSTM(input, (h, c))\n",
        "            output = self.output_layer((torch.nn.Dropout(p=0.3))(h))\n",
        "            sentences[:, t] = output\n",
        "            weights[:, t] = alpha\n",
        "\n",
        "            if(not self.training):\n",
        "                embedded_caps = self.embedding_layer(output.max(1)[1].reshape(feats.size(0), 1))\n",
        "\n",
        "        return weights, sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "jr2zaeP_xRjN",
        "outputId": "f5c4e0f2-24b3-4f50-fd51-7a77f3d90cfe"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "vocab_size = train_loader.dataset.TN.getVocabularyLength()\n",
        "EnNet = ENet().to(device)\n",
        "DeNet = DNet(vocab_size).to(device)\n",
        "EnNet.load_state_dict(torch.load(open(\"/content/drive/MyDrive/Data/encoder.pth\",\"rb\")))\n",
        "DeNet.load_state_dict(torch.load(open(\"/content/drive/MyDrive/Data/decoder.pth\",\"rb\")))\n",
        "\n",
        "optimizer = torch.optim.Adam(DeNet.parameters(), lr=4e-4)\n",
        "loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "EnNet.eval()\n",
        "DeNet.train()\n",
        "\n",
        "for epoch in range(1,NUM_EPOCHS+1):\n",
        "    count = 0\n",
        "    lo = 0\n",
        "    for img, cap, caps in train_loader:\n",
        "        img, cap = torch.autograd.Variable(img).to(device), torch.autograd.Variable(cap).to(device)\n",
        "        feats = EnNet(img)\n",
        "        optimizer.zero_grad()\n",
        "        weights, sentences = DeNet(feats, cap)\n",
        "        targets = pack_padded_sequence(cap[:,1:], [len(caption)-1 for caption in cap[:,1:]], batch_first=True)[0]\n",
        "        sentences = pack_padded_sequence(sentences, [len(sentence)-1 for sentence in sentences], batch_first=True)[0]\n",
        "        print(targets.size(), sentences.size(), cap.size())\n",
        "        att_regularization = 0.5*torch.mean(((1-torch.sum(weights, dim=1))**2))\n",
        "        L = loss(sentences, cap)\n",
        "        L += att_regularization\n",
        "        L.backward()\n",
        "        optimizer.step()\n",
        "        lo += L.item()\n",
        "        count += 1\n",
        "\n",
        "    lo /= count\n",
        "    print(\"Epoch {}: Loss: {}\".format(epoch, lo))\n",
        "    torch.save(DeNet.state_dict(), open(\"/content/drive/MyDrive/Data/decoder.pth\",\"wb\"))\n",
        "    torch.save(EnNet.state_dict(), open(\"/content/drive/MyDrive/Data/encoder.pth\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3968]) torch.Size([3968, 8724]) torch.Size([128, 33])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-22614235c911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0matt_regularization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0matt_regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m         raise ValueError(\n\u001b[0;32m-> 2385\u001b[0;31m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3968) to match target batch_size (128)."
          ]
        }
      ]
    }
  ]
}