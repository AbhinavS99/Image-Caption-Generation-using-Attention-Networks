{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvEiMRphigkQ",
    "outputId": "1f2be2bc-e05d-427c-975d-6c7edc8fb591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as T\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from google.colab import drive\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextNumericalizer():\n",
    "\tdef __init__(self, vocab, tokenizer):\n",
    "\t\tself.vocab = vocab\n",
    "\n",
    "\tdef tokenize(self, sentence):\n",
    "\t\treturn (re.sub(r'[^\\w\\s]', '', sentence).lower()).split(\" \")\n",
    "\n",
    "\tdef SentenceToVector(self, sentence):\n",
    "\t\treturn [self.vocab.stoi[token.lower()] for token in self.tokenize(sentence)]\n",
    "\n",
    "\tdef VectorToSentence(self, vector):\n",
    "\t\treturn [self.vocab.itos[integer] for integer in vector]\n",
    "  \n",
    "\tdef getVocabularyLength(self):\n",
    "\t\treturn len(self.vocab.stoi.keys())\n",
    "\n",
    "class ImgCapDataset(Dataset):\n",
    "    def __init__(self, X, Y, TN, transform=None):\n",
    "        assert len(X)==len(Y), \"Data should be of the same length! [Error: X(\" + str(len(X)) + \") != Y(\" + str(lenY) + \")]\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.TN = TN\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        temp = deepcopy(self.Y[index])\n",
    "        for i in range(len(self.Y[index])):\n",
    "            temp[i] = re.sub(r'[^\\w\\s]', '', temp[i]).lower()\n",
    "\n",
    "        return self.transform(self.X[index]), torch.tensor([self.TN.vocab.stoi[\"<sos>\"]]+self.TN.SentenceToVector(temp[0])+[self.TN.vocab.stoi[\"<eos>\"]], dtype=torch.long), [self.TN.SentenceToVector(i) for i in temp]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "class CollateFunction():\n",
    "    def __init__(self, padding):\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return torch.cat([i[0].unsqueeze(0) for i in batch], dim=0), pad_sequence([i[1] for i in batch], batch_first=True, padding_value=self.padding), [i[2] for i in batch]\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#val_loader = pkl.load(open(\"/content/drive/MyDrive/Data/Val/val_loader.pkl\",\"rb\"))\n",
    "#test_loader = pkl.load(open(\"/content/drive/MyDrive/Data/Test/test_loader.pkl\",\"rb\"))\n",
    "\n",
    "# Encoder Network\n",
    "class ENet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ENet, self).__init__()\n",
    "        temp = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "        # No Fine-Tuning Of The Pre-Trained Model\n",
    "        for p in temp.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*list(temp.features.children())[:-2])\n",
    "\n",
    "        for c in list(self.encoder.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        self.feats = torch.nn.AdaptiveAvgPool2d((14,14))\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        enc = self.feats(enc)\n",
    "        enc = enc.permute(0, 2, 3, 1)\n",
    "        enc = enc.view(-1, 196, 512)\n",
    "\n",
    "        return enc\n",
    "\n",
    "# Bahdanau Attention Network\n",
    "class ANet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANet, self).__init__()\n",
    "        self.v = torch.nn.Linear(512, 1)\n",
    "        self.W = torch.nn.Linear(512, 512)\n",
    "        self.U = torch.nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, feats, h_state):\n",
    "        U = self.U(h_state).unsqueeze(dim=1)\n",
    "        W = self.W(feats)\n",
    "        A = torch.nn.functional.tanh(W+U)\n",
    "        e = self.v(A).squeeze(dim=2)\n",
    "        alpha = torch.nn.functional.softmax(e, dim=1)\n",
    "        context_vector = (feats*alpha.unsqueeze(dim=2)).sum(dim=1)\n",
    "\n",
    "        return alpha, context_vector\n",
    "\n",
    "# Decoder Network With Bahdanau Attention Mechanism\n",
    "class DNet(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(DNet, self).__init__()\n",
    "        self.v_size = vocab_size\n",
    "        self.ANet = ANet()\n",
    "        self.init_h = torch.nn.Linear(512, 512)\n",
    "        self.init_c = torch.nn.Linear(512, 512)\n",
    "        self.f_beta = torch.nn.Linear(512, 512)\n",
    "        self.output_layer = torch.nn.Linear(512, vocab_size)\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, 512)\n",
    "        self.LSTM = torch.nn.LSTMCell(1024, 512)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        avg = torch.mean(feats, dim=1)\n",
    "        h = torch.nn.functional.tanh(self.init_h(avg))\n",
    "        c = torch.nn.functional.tanh(self.init_c(avg))\n",
    "        T = len(caps[0])-1\n",
    "        embedded_caps = None\n",
    "\n",
    "        if(self.training):\n",
    "            embedded_caps = self.embedding_layer(caps)\n",
    "        else:\n",
    "            embedded_caps = self.embedding_layer(torch.zeros(feats.size(0), 1).long().to(device))\n",
    "\n",
    "        sentences = torch.zeros((feats.size(0), T, self.v_size)).to(device)\n",
    "        weights = torch.zeros((feats.size(0), T, 196)).to(device)\n",
    "\n",
    "        for t in range(T):\n",
    "            alpha, context_vector = self.ANet(feats, h)\n",
    "            gated_context_vector = torch.nn.functional.sigmoid(self.f_beta(h))*context_vector\n",
    "            input = None\n",
    "\n",
    "            if(self.training):\n",
    "                input = torch.cat((embedded_caps[:, t], gated_context_vector), dim=1)\n",
    "            else:\n",
    "                embedded_caps = embedded_caps.squeeze(1) if embedded_caps.dim()==3 else embedded_caps\n",
    "                input = torch.cat((embedded_caps, gated_context_vector), dim=1)\n",
    "\n",
    "            h, c = self.LSTM(input, (h, c))\n",
    "            output = self.output_layer((torch.nn.Dropout(p=0.3))(h))\n",
    "            sentences[:, t] = output\n",
    "            weights[:, t] = alpha\n",
    "\n",
    "            if(not self.training):\n",
    "                embedded_caps = self.embedding_layer(output.max(1)[1].reshape(feats.size(0), 1))\n",
    "\n",
    "        return weights, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLcXWaOM9Cey",
    "outputId": "67efab86-c266-45f4-83ae-981eb2144147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEWM1Im7HPHr",
    "outputId": "970adf20-2fd2-430c-d0c8-02cd79ecfce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "-6d7b-1ug6P3",
    "outputId": "ef1e4960-adf7-4e68-f642-102beb07d571"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nEnNet.eval()\\nDeNet.eval()\\n\\nval_bleu = []\\nval_meteor = 0\\n\\nwith torch.no_grad():\\n    hypotheses = []\\n    references = []\\n\\n    for img, cap, caps in val_loader:\\n        img, cap = torch.autograd.Variable(img).to(device), torch.autograd.Variable(cap).to(device)\\n        feats = EnNet(img)\\n        weights, sentences = DeNet(feats, cap)\\n\\n        references.append([val_loader.dataset.TN.VectorToSentence(c) for c in caps[0]])\\n        hypotheses.append(val_loader.dataset.TN.VectorToSentence([token for token in torch.max(sentences, dim=2)[1][0] if token!=val_loader.dataset.TN.vocab.stoi[\"<sos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<eos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<pad>\"]]))\\n\\n        val_meteor += meteor_score([\" \".join([ch for ch in r]) for r in references[-1]], \" \".join([ch for ch in hypotheses[-1]]))\\n\\nval_meteor /= len(val_loader.dataset)\\nval_bleu.append(corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0)))\\nval_bleu.append(corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0)))\\nval_bleu.append(corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0)))\\nval_bleu.append(corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)))\\nprint(\"Validation Set: BLEU Scores: 1: {}, 2: {}, 3: {}, 4: {},\".format(val_bleu[0], val_bleu[1], val_bleu[2], val_bleu[3]))\\nprint(\"Validation Set: Meteor Score: {}\".format(val_meteor))\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "vocab_size = val_loader.dataset.TN.getVocabularyLength()\n",
    "EnNet = ENet().to(device)\n",
    "DeNet = DNet(vocab_size).to(device)\n",
    "EnNet.load_state_dict(torch.load(open(\"/content/drive/MyDrive/Data/encoder.pth\",\"rb\")))\n",
    "DeNet.load_state_dict(torch.load(open(\"/content/drive/MyDrive/Data/decoder.pth\",\"rb\")))\n",
    "\n",
    "EnNet.eval()\n",
    "DeNet.eval()\n",
    "\n",
    "val_bleu = []\n",
    "val_meteor = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "\n",
    "    for img, cap, caps in val_loader:\n",
    "        img, cap = torch.autograd.Variable(img).to(device), torch.autograd.Variable(cap).to(device)\n",
    "        feats = EnNet(img)\n",
    "        weights, sentences = DeNet(feats, cap)\n",
    "\n",
    "        references.append([val_loader.dataset.TN.VectorToSentence(c) for c in caps[0]])\n",
    "        hypotheses.append(val_loader.dataset.TN.VectorToSentence([token for token in torch.max(sentences, dim=2)[1][0] if token!=val_loader.dataset.TN.vocab.stoi[\"<sos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<eos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<pad>\"]]))\n",
    "\n",
    "        val_meteor += meteor_score([\" \".join([ch for ch in r]) for r in references[-1]], \" \".join([ch for ch in hypotheses[-1]]))\n",
    "\n",
    "val_meteor /= len(val_loader.dataset)\n",
    "val_bleu.append(corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0)))\n",
    "val_bleu.append(corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0)))\n",
    "val_bleu.append(corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0)))\n",
    "val_bleu.append(corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"Validation Set: BLEU Scores: 1: {}, 2: {}, 3: {}, 4: {},\".format(val_bleu[0], val_bleu[1], val_bleu[2], val_bleu[3]))\n",
    "print(\"Validation Set: Meteor Score: {}\".format(val_meteor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33GCzVxoIFL0",
    "outputId": "8b5d5e17-9931-421e-f95d-b1ddb0aecee7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: BLEU Scores: 1: 0.5434922977585094, 2: 0.31738726576058, 3: 0.1954854887146144, 4: 0.12069149020351676,\n",
      "Test Set: Meteor Score: 0.31989724270647407\n"
     ]
    }
   ],
   "source": [
    "EnNet.eval()\n",
    "DeNet.eval()\n",
    "\n",
    "test_bleu = []\n",
    "test_meteor = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "\n",
    "    for img, cap, caps in test_loader:\n",
    "        img, cap = torch.autograd.Variable(img).to(device), torch.autograd.Variable(cap).to(device)\n",
    "        feats = EnNet(img)\n",
    "        weights, sentences = DeNet(feats, cap)\n",
    "\n",
    "        references.append([test_loader.dataset.TN.VectorToSentence(c) for c in caps[0]])\n",
    "        hypotheses.append(test_loader.dataset.TN.VectorToSentence([token for token in torch.max(sentences, dim=2)[1][0] if token!=val_loader.dataset.TN.vocab.stoi[\"<sos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<eos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<pad>\"]]))\n",
    "\n",
    "        test_meteor += meteor_score([\" \".join([ch for ch in r]) for r in references[-1]], \" \".join([ch for ch in hypotheses[-1]]))\n",
    "\n",
    "test_meteor /= len(test_loader.dataset)\n",
    "test_bleu.append(corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0)))\n",
    "test_bleu.append(corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0)))\n",
    "test_bleu.append(corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0)))\n",
    "test_bleu.append(corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"Test Set: BLEU Scores: 1: {}, 2: {}, 3: {}, 4: {},\".format(test_bleu[0], test_bleu[1], test_bleu[2], test_bleu[3]))\n",
    "print(\"Test Set: Meteor Score: {}\".format(test_meteor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gzeaOc7jNcBD"
   },
   "outputs": [],
   "source": [
    "train_loader = pkl.load(open(\"/content/drive/MyDrive/Data/Train/eval_train_loader.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EVWwVjsNgte",
    "outputId": "43557086-8b77-4341-a43c-e1a8b9f9217f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: BLEU Scores: 1: 0.7033579613325665, 2: 0.5697779255252082, 3: 0.4985996175006812, 4: 0.4498096499723183,\n",
      "Train Set: Meteor Score: 0.5389961187733419\n"
     ]
    }
   ],
   "source": [
    "EnNet.eval()\n",
    "DeNet.eval()\n",
    "\n",
    "train_bleu = []\n",
    "train_meteor = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "\n",
    "    for img, cap, caps in train_loader:\n",
    "        img, cap = torch.autograd.Variable(img).to(device), torch.autograd.Variable(cap).to(device)\n",
    "        feats = EnNet(img)\n",
    "        weights, sentences = DeNet(feats, cap)\n",
    "\n",
    "        references.append([train_loader.dataset.TN.VectorToSentence(c) for c in caps[0]])\n",
    "        hypotheses.append(train_loader.dataset.TN.VectorToSentence([token for token in torch.max(sentences, dim=2)[1][0] if token!=val_loader.dataset.TN.vocab.stoi[\"<sos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<eos>\"] and token!=val_loader.dataset.TN.vocab.stoi[\"<pad>\"]]))\n",
    "\n",
    "        train_meteor += meteor_score([\" \".join([ch for ch in r]) for r in references[-1]], \" \".join([ch for ch in hypotheses[-1]]))\n",
    "\n",
    "train_meteor /= len(train_loader.dataset)\n",
    "train_bleu.append(corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0)))\n",
    "train_bleu.append(corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0)))\n",
    "train_bleu.append(corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0)))\n",
    "train_bleu.append(corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"Train Set: BLEU Scores: 1: {}, 2: {}, 3: {}, 4: {},\".format(train_bleu[0], train_bleu[1], train_bleu[2], train_bleu[3]))\n",
    "print(\"Train Set: Meteor Score: {}\".format(train_meteor))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
