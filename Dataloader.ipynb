{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataloader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYq2sxAv00RP",
        "outputId": "578793a3-8210-4f76-d339-64b18f257f67"
      },
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "from torchtext.vocab import Vocab\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.transforms as T\n",
        "import pickle as pkl\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "\n",
        "class TextNumericalizer():\n",
        "\tdef __init__(self, vocab, tokenizer):\n",
        "\t\tself.vocab = vocab\n",
        "\n",
        "\tdef tokenize(self, sentence):\n",
        "\t\treturn (re.sub(r'[^\\w\\s]', '', sentence).lower()).split(\" \")\n",
        "\n",
        "\tdef SentenceToVector(self, sentence):\n",
        "\t\treturn [self.vocab.stoi[token.lower()] for token in self.tokenize(sentence)]\n",
        "\n",
        "\tdef VectorToSentence(self, vector):\n",
        "\t\treturn [self.vocab.itos[integer] for integer in vector]\n",
        "  \n",
        "\tdef getVocabularyLength(self):\n",
        "\t\treturn len(self.vocab.stoi.keys())\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "TN = pkl.load(open(\"/content/drive/MyDrive/Data/TextNumericalizer.pkl\",\"rb\"))\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "class ImgCapDataset(Dataset):\n",
        "    def __init__(self, X, Y, TN, transform=None):\n",
        "        assert len(X)==len(Y), \"Data should be of the same length! [Error: X(\" + str(len(X)) + \") != Y(\" + str(lenY) + \")]\"\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.TN = TN\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        temp = deepcopy(self.Y[index])\n",
        "        for i in range(len(self.Y[index])):\n",
        "            temp[i] = re.sub(r'[^\\w\\s]', '', temp[i]).lower()\n",
        "\n",
        "        return self.transform(self.X[index]), torch.tensor([self.TN.vocab.stoi[\"<sos>\"]]+self.TN.SentenceToVector(temp[0])+[self.TN.vocab.stoi[\"<eos>\"]], dtype=torch.long), [self.TN.SentenceToVector(i) for i in temp]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "class CollateFunction():\n",
        "    def __init__(self, padding):\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return torch.cat([i[0].unsqueeze(0) for i in batch], dim=0), pad_sequence([i[1] for i in batch], batch_first=True, padding_value=self.padding), [i[2] for i in batch]\n",
        "\n",
        "train_data_captions = pkl.load(open(\"/content/drive/MyDrive/Data/Train/train_captions.pkl\",\"rb\"))\n",
        "train_images = []\n",
        "train_captions = []\n",
        "\n",
        "for img_name in train_data_captions:\n",
        "    train_captions.append(train_data_captions[img_name])\n",
        "    train_images.append(Image.open(\"/content/drive/MyDrive/Data/Train/Images/\"+str(img_name)))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ImgCapDataset(train_images, train_captions, TN, transform=T.Compose([T.Resize((224, 224)), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=CollateFunction(TN.vocab.stoi[\"<pad>\"]),\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        "                          )\n",
        "\n",
        "pkl.dump(train_loader, open(\"/content/drive/MyDrive/Data/Train/eval_train_loader.pkl\",\"wb\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}